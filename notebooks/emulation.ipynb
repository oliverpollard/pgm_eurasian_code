{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17d47785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 22:35:18.773241: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.stats import uniform, norm\n",
    "from itertools import combinations\n",
    "\n",
    "from emulation import GP, loo\n",
    "from icepy import volume_to_sle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80202603",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiGP:\n",
    "    def __init__(self, X_data, Y_data, da_template=None):\n",
    "        shape = Y_data.shape[1:]\n",
    "        n_samples = len(Y_data)\n",
    "        Y_data = Y_data.reshape(n_samples, -1)\n",
    "        \n",
    "        gps = np.empty(Y_data.shape[-1],dtype=object)\n",
    "        for idx in range(Y_data.shape[-1]):\n",
    "            gps[idx] = GP.from_data(\n",
    "                X_data = X_data,\n",
    "                Y_data = Y_data[:,idx].reshape(-1,1)\n",
    "            )\n",
    "            \n",
    "        self.gps = gps.flatten()\n",
    "        self.shape = shape\n",
    "        self.da_template = da_template\n",
    "        \n",
    "    def train(self):\n",
    "        for gp in tqdm(self.gps):\n",
    "            gp.train()\n",
    "            \n",
    "    def predict(self, X):\n",
    "        mean, var = np.zeros((len(X), len(self.gps))), np.zeros((len(X), len(self.gps)))\n",
    "        for idx, gp in enumerate(self.gps):\n",
    "            mean_temp, var_temp = gp.predict(X)\n",
    "            mean[:,idx], var[:,idx] = mean_temp.flatten(), var_temp.flatten()\n",
    "        mean, var = mean.reshape(len(X), *self.shape), var.reshape(len(X),*self.shape)\n",
    "        if self.da_template is not None:\n",
    "            mean_da, var_da = xr.ones_like(self.da_template), xr.ones_like(self.da_template)\n",
    "            mean_da = mean_da.expand_dims(dim={\"predict_idx\": np.arange(len(X))}, axis=0) * mean\n",
    "            var_da = var_da.expand_dims(dim={\"predict_idx\": np.arange(len(X))}, axis=0) * var\n",
    "            return mean_da, var_da\n",
    "        else:\n",
    "            return mean, var\n",
    "        \n",
    "\n",
    "class MultiGPImp(MultiGP):\n",
    "    def __init__(self, X_data, Y_data, metric_ds, da_template=None):\n",
    "        super().__init__(X_data, Y_data, da_template=da_template)\n",
    "        self.metric_ds = metric_ds\n",
    "    \n",
    "    def calc_imp(self, X, fudge_factor=1, return_full_imp=False):\n",
    "        m_predict, m_var = self.predict(X)\n",
    "        imp_numerator = (m_predict - self.metric_ds.metric_obs)**2\n",
    "        imp_denominator = ((self.metric_ds.metric_sim_var + self.metric_ds.metric_obs_var)*fudge_factor + m_var)\n",
    "        imp = np.sqrt(imp_numerator/imp_denominator)\n",
    "        imp_mean = imp.mean(dim=[\"metric\", \"model\", \"time\"])\n",
    "        \n",
    "        if return_full_imp is True:\n",
    "            return imp_mean, imp\n",
    "        else:\n",
    "            return imp_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941f9114",
   "metadata": {},
   "source": [
    "# PGM Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb4597db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove failed runs\n",
    "pgm_ens_members = np.delete(np.arange(1000), 613)\n",
    "pgm_ds = xr.open_dataset(\"../pd_ens/pgm_topo1.nc\").sel(ensemble_idx=pgm_ens_members)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e65ba126",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = pgm_ds.x.values, pgm_ds.y.values\n",
    "dA = np.abs(x[1] - x[0]) * np.abs(y[1] - y[0])\n",
    "pgm_volume_da = pgm_ds.ice_thickness.sum(dim=[\"y\", \"x\"]) * dA\n",
    "pgm_volume_sle_da = volume_to_sle(pgm_volume_da)\n",
    "pgm_max_thickness_da = pgm_ds.ice_thickness.max(dim=[\"y\", \"x\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96b33a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pgm_metrics_ds = xr.merge(\n",
    "    [\n",
    "        pgm_volume_da.rename(\"volume\"),\n",
    "        pgm_volume_sle_da.rename(\"volume_sle\"),\n",
    "        pgm_max_thickness_da.rename(\"max_thickness\")\n",
    "    ]\n",
    ")\n",
    "pgm_metrics_ds.to_netcdf(\"pgm_metrics1.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bdd172",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f721886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_sample_df = pd.read_csv(\"../../data/param_sample/ld_lhs_values.csv\")\n",
    "ld_metrics_ds = xr.open_dataset(\"ld_metrics.nc\")\n",
    "\n",
    "# remove failed runs\n",
    "pgm_ens_members = np.delete(np.arange(1000), 613)\n",
    "pgm_sample_df = pd.read_csv(\"../pd_ens/sample.csv\", index_col=0).iloc[pgm_ens_members]\n",
    "# rename parameter to match ld\n",
    "pgm_sample_df.rename({\"g_pgm_ice_streams_ice_stream\": \"g_lgm_ice_streams_ice_stream\"}, inplace=True, axis=1)\n",
    "pgm_metrics_ds = xr.open_dataset(\"pgm_metrics1.nc\").sel(ensemble_idx=pgm_ens_members)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161a9f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pgm_mod_ice_ds = xr.open_dataset(\"../pd_ens/pgm_topo0.nc\").ice_thickness.sel(ensemble_idx = pgm_ens_members)\n",
    "pgm_mod_vol = volume_to_sle((pgm_mod_ice_ds * 5000**2).sum(dim=[\"x\", \"y\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145a7d70",
   "metadata": {},
   "source": [
    "# Train Emulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40b8ed69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426fa83af0b14ec78f8bdffb75113fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# implausibility emulator\n",
    "imp_gp = MultiGPImp(\n",
    "    X_data = ld_sample_df.values,\n",
    "    Y_data = ld_metrics_ds.metric_sim_debiased.values,\n",
    "    da_template = ld_metrics_ds.metric_sim.sel(ensemble_index=0),\n",
    "    metric_ds = ld_metrics_ds,\n",
    ")\n",
    "imp_gp.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe54cd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "pgm_mod_vol_Y = pgm_mod_vol.values.reshape(-1,1)\n",
    "pgm_mod_vol_gp = GP.from_data(\n",
    "    X_data = pgm_sample_df.values,\n",
    "    Y_data = pgm_mod_vol_Y\n",
    ")\n",
    "pgm_mod_vol_gp.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0b0e245",
   "metadata": {},
   "outputs": [],
   "source": [
    "glac1d_20_vol_Y = volume_to_sle(ld_metrics_ds.metric_sim_debiased.sel(time=-20000, model=\"glac1d\").sum(dim=\"metric\").values).reshape(-1,1)\n",
    "glac1d_20_vol_gp = GP.from_data(\n",
    "    X_data = ld_sample_df.values,\n",
    "    Y_data = glac1d_20_vol_Y\n",
    ")\n",
    "glac1d_20_vol_gp.train()\n",
    "\n",
    "ice6g_22_vol_Y = volume_to_sle(ld_metrics_ds.metric_sim_debiased.sel(time=-22000, model=\"ice6g\").sum(dim=\"metric\").values).reshape(-1,1)\n",
    "ice6g_22_vol_gp = GP.from_data(\n",
    "    X_data = ld_sample_df.values,\n",
    "    Y_data = ice6g_22_vol_Y\n",
    ")\n",
    "ice6g_22_vol_gp.train()\n",
    "\n",
    "pgm_vol_Y = pgm_metrics_ds.volume_sle.values.reshape(-1,1)\n",
    "pgm_vol_gp = GP.from_data(\n",
    "    X_data = pgm_sample_df.values,\n",
    "    Y_data = pgm_vol_Y\n",
    ")\n",
    "pgm_vol_gp.train()\n",
    "\n",
    "pgm_max_thickness_Y = pgm_metrics_ds.max_thickness.values.reshape(-1,1)\n",
    "pgm_max_thickness_gp = GP.from_data(\n",
    "    X_data = pgm_sample_df.values,\n",
    "    Y_data = pgm_max_thickness_Y\n",
    ")\n",
    "pgm_max_thickness_gp.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a19dad3",
   "metadata": {},
   "source": [
    "# Large Random Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aeef72a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load last deglac dists\n",
    "with open(\"../ld_ens/problem.json\", \"r\") as f:\n",
    "    ld_problem = json.load(f)\n",
    "    \n",
    "ld_problem['names'][0] = 'g_lgm_ice_streams_ice_stream'    \n",
    "ld_ranges = {ld_problem['names'][idx]: ld_problem['bounds'][idx] for idx in range(len(ld_problem['names']))}\n",
    "ld_dists = {name: uniform(loc=bound[0], scale=bound[1]-bound[0]) for name, bound in ld_ranges.items()}\n",
    "\n",
    "# load pgm dists\n",
    "with open(\"../pd_ens/problem.json\", \"r\") as f:\n",
    "    pgm_problem = json.load(f)\n",
    "\n",
    "# modify to make margin parameter normal dist\n",
    "pgm_problem['names'][0] = 'g_lgm_ice_streams_ice_stream'\n",
    "pgm_problem['dists'] = pgm_problem['num_vars']*['unif']\n",
    "margin_perc_idx = pgm_problem['names'].index('margin_perc')\n",
    "pgm_problem['dists'][margin_perc_idx] = 'norm'\n",
    "pgm_problem['bounds'][margin_perc_idx] = [0.5, 0.1]\n",
    "\n",
    "pgm_ranges = {pgm_problem['names'][idx]: pgm_problem['bounds'][idx] for idx in range(len(pgm_problem['names']))}\n",
    "pgm_dists = {name: uniform(loc=bound[0], scale=bound[1]-bound[0]) for name, bound in pgm_ranges.items()}\n",
    "pgm_dists['margin_perc'] = norm(loc=0.5, scale=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0580a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = int(1e5)\n",
    "X_ld_rand = pd.DataFrame({name: dist.rvs(N) for name, dist in ld_dists.items()})\n",
    "X_pgm_rand = pd.DataFrame({name: dist.rvs(N) for name, dist in pgm_dists.items()})\n",
    "X_ld_rand.to_csv(\"ld_predict_sample.csv\")\n",
    "X_pgm_rand.to_csv(\"pgm_predict_sample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4093675",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "13fbd057",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ld_regions_mean, pred_ld_regions_var = imp_gp.predict(\n",
    "    X_ld_rand[ld_sample_df.columns].values,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "137123f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ld_regions_ds = xr.merge(\n",
    "    [\n",
    "        pred_ld_regions_mean.rename(\"metric_sim_mean\"),\n",
    "        pred_ld_regions_var.rename(\"metric_sim_var\")\n",
    "    ]\n",
    ")\n",
    "pred_ld_regions_ds.to_netcdf(\"ld_predict_regions.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "d24a9975",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ld_imp = imp_gp.calc_imp(\n",
    "    X_ld_rand[ld_sample_df.columns].values,\n",
    "    fudge_factor=1.2\n",
    ")\n",
    "pred_ld_nroy = pred_ld_imp < 3\n",
    "\n",
    "pred_glac1d_20_vol, pred_glac1d_20_vol_var = glac1d_20_vol_gp.predict(X_ld_rand.values)\n",
    "pred_ice6g_22_vol, pred_ice6g_22_vol_var = ice6g_22_vol_gp.predict(X_ld_rand.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "83d87d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = {\"predict_index\": np.arange(N)}\n",
    "pred_ld_ds = xr.merge(\n",
    "    [\n",
    "        xr.DataArray(\n",
    "            pred_glac1d_20_vol.flatten(), \n",
    "            coords=coords\n",
    "        ).rename(\"glac1d_20_volume_mean\"),\n",
    "        xr.DataArray(\n",
    "            pred_ice6g_22_vol.flatten(), \n",
    "            coords=coords\n",
    "        ).rename(\"ice6g_22_volume_mean\"),\n",
    "        xr.DataArray(\n",
    "            pred_glac1d_20_vol_var.flatten(), \n",
    "            coords=coords\n",
    "        ).rename(\"glac1d_20_volume_var\"),\n",
    "        xr.DataArray(\n",
    "            pred_ice6g_22_vol_var.flatten(), \n",
    "            coords=coords\n",
    "        ).rename(\"ice6g_22_volume_var\"),\n",
    "        xr.DataArray(\n",
    "            pred_ld_imp.values, \n",
    "            coords=coords\n",
    "        ).rename(\"implausibility\"),\n",
    "        xr.DataArray(\n",
    "            pred_ld_nroy.values, \n",
    "            coords=coords\n",
    "        ).rename(\"nroy\")\n",
    "    ]\n",
    ")\n",
    "pred_ld_ds.to_netcdf(\"ld_predict.nc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "a2ba968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_pgm_sample_imp = imp_gp.calc_imp(\n",
    "    pgm_sample_df[ld_sample_df.columns].values,\n",
    "    fudge_factor=1.2\n",
    ")\n",
    "pred_pgm_sample_nroy = pred_pgm_sample_imp < 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "ba547422",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = {\"ensemble_index\": pgm_sample_df.index}\n",
    "pgm_imp_ds = xr.merge(\n",
    "    [\n",
    "        xr.DataArray(\n",
    "            pred_pgm_sample_imp.values.flatten(), \n",
    "            coords=coords\n",
    "        ).rename(\"implausibility\"),\n",
    "        xr.DataArray(\n",
    "            pred_pgm_sample_nroy.values.flatten(),\n",
    "            coords=coords\n",
    "        ).rename(\"nroy\"),\n",
    "    ]\n",
    ")\n",
    "pgm_imp_ds.to_netcdf(\"pgm_implausibility.nc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "6c8fd3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_pgm_imp = imp_gp.calc_imp(\n",
    "    X_pgm_rand[ld_sample_df.columns].values,\n",
    "    fudge_factor=1.2\n",
    ")\n",
    "pred_pgm_nroy = pred_pgm_imp < 3\n",
    "\n",
    "pred_pgm_vol, pred_pgm_vol_var = pgm_vol_gp.predict(X_pgm_rand.values)\n",
    "pred_pgm_max_thickness, pred_pgm_max_thickness_var = pgm_max_thickness_gp.predict(X_pgm_rand.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "df8b4ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias chosen since 20ka is largest simulated volume slice\n",
    "#Â for glac and ice6g margins, we take the mean and scale\n",
    "# by the mean ice volume from the large sample\n",
    "ld_volume = ld_metrics_ds.metric_sim.sum(dim=\"metric\").mean(dim=\"ensemble_index\")\n",
    "ld_volume_bias = ld_metrics_ds.metric_sim_bias.sum(dim=\"metric\")\n",
    "ld_volume_bias_perc = ld_volume_bias/ld_volume\n",
    "mean_bias_perc = ld_volume_bias_perc.sel(time=-20000).mean().values\n",
    "\n",
    "pgm_bias = pred_pgm_vol.mean() * mean_bias_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "f87fabf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_pgm_vol_debiased = pred_pgm_vol - pgm_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "fa59bb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = {\"predict_index\": np.arange(N)}\n",
    "pred_pgm_ds = xr.merge(\n",
    "    [\n",
    "        xr.DataArray(\n",
    "            pred_pgm_vol_debiased.flatten(), \n",
    "            coords=coords\n",
    "        ).rename(\"volume_mean_debiased\"),\n",
    "        xr.DataArray(\n",
    "            pgm_bias,\n",
    "        ).rename(\"volume_bias\"),\n",
    "        xr.DataArray(\n",
    "            mean_bias_perc,\n",
    "        ).rename(\"volume_bias_perc\"),\n",
    "        xr.DataArray(\n",
    "            pred_pgm_vol.flatten(), \n",
    "            coords=coords\n",
    "        ).rename(\"volume_mean\"),\n",
    "        xr.DataArray(\n",
    "            pred_pgm_vol_var.flatten(), \n",
    "            coords=coords\n",
    "        ).rename(\"volume_var\"),\n",
    "        xr.DataArray(\n",
    "            pred_pgm_max_thickness.flatten(), \n",
    "            coords=coords\n",
    "        ).rename(\"max_thickness_mean\"),\n",
    "        xr.DataArray(\n",
    "            pred_pgm_max_thickness_var.flatten(), \n",
    "            coords=coords\n",
    "        ).rename(\"max_thickness_var\"),\n",
    "        xr.DataArray(\n",
    "            pred_pgm_imp.values, \n",
    "            coords=coords\n",
    "        ).rename(\"implausibility\"),\n",
    "        xr.DataArray(\n",
    "            pred_pgm_nroy.values, \n",
    "            coords=coords\n",
    "        ).rename(\"nroy\")\n",
    "    ]\n",
    ")\n",
    "pred_pgm_ds.to_netcdf(\"pgm_predict.nc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "91fd6f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_pgm_mod_vol, pred_pgm_mod_vol_var = pgm_mod_vol_gp.predict(X_pgm_rand.values)\n",
    "pred_pgm_mod_vol_debiased = pred_pgm_mod_vol - pgm_bias\n",
    "\n",
    "coords = {\"predict_index\": np.arange(N)}\n",
    "pred_pgm_mod_ds = xr.merge(\n",
    "    [\n",
    "        xr.DataArray(\n",
    "            pred_pgm_mod_vol_debiased.flatten(), \n",
    "            coords=coords\n",
    "        ).rename(\"mod_volume_mean_debiased\"),\n",
    "        xr.DataArray(\n",
    "            pred_pgm_mod_vol_var.flatten(), \n",
    "            coords=coords\n",
    "        ).rename(\"mod_volume_var\"),\n",
    "    ]\n",
    ")\n",
    "pred_pgm_mod_ds.to_netcdf(\"pgm_mod_predict.nc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f5183c",
   "metadata": {},
   "source": [
    "# LD Optical Depth Panels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ebe4831",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params = len(ld_ranges)\n",
    "panel_res = 40\n",
    "sample_res = 1000\n",
    "param_combs = list(combinations(list(ld_ranges.keys()), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f32086b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84207c9fe93f4c8dadb7d4383a007a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff27ce46c82d4a19aa8cfc0de6118d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "density_panels = np.zeros((len(param_combs),panel_res, panel_res))\n",
    "mean_panels = np.zeros((len(param_combs),panel_res, panel_res))\n",
    "min_panels = np.zeros((len(param_combs),panel_res, panel_res))\n",
    "max_panels = np.zeros((len(param_combs),panel_res, panel_res))\n",
    "\n",
    "for panel_idx, (p0_name, p1_name) in enumerate(tqdm(param_combs)):\n",
    "    dists_subset = ld_dists.copy()\n",
    "    dists_subset.pop(p0_name)\n",
    "    dists_subset.pop(p1_name)\n",
    "\n",
    "    p0 = np.linspace(*ld_ranges[p0_name], panel_res)\n",
    "    p1 = np.linspace(*ld_ranges[p1_name], panel_res)\n",
    "\n",
    "    panel = np.zeros((panel_res, panel_res, sample_res))\n",
    "\n",
    "    for p0_idx in tqdm(range(panel_res), leave=False):\n",
    "        for p1_idx in range(panel_res):\n",
    "\n",
    "            panel_sample = pd.DataFrame({name: dist.rvs(sample_res) for name, dist in dists_subset.items()})\n",
    "            panel_sample[p0_name] = p0[p0_idx]\n",
    "            panel_sample[p1_name] = p1[p1_idx]\n",
    "            imp_pred = imp_gp.calc_imp(\n",
    "                panel_sample[list(ld_sample_df.columns)].values,\n",
    "                fudge_factor=1.2\n",
    "            )\n",
    "            panel[p0_idx, p1_idx] = imp_pred.values\n",
    "            \n",
    "    density_panels[panel_idx] = np.sum(panel <= 3, axis=2)\n",
    "    mean_panels[panel_idx] = np.mean(panel, axis=2)\n",
    "    min_panels[panel_idx] = np.min(panel, axis=2)\n",
    "    max_panels[panel_idx] = np.max(panel, axis=2)\n",
    "    \n",
    "density_panels = density_panels/sample_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867d5764",
   "metadata": {},
   "outputs": [],
   "source": [
    "density_panels_2d = np.zeros((n_params, n_params, panel_res, panel_res))\n",
    "mean_panels_2d = np.zeros((n_params, n_params, panel_res, panel_res))\n",
    "min_panels_2d = np.zeros((n_params, n_params, panel_res, panel_res))\n",
    "max_panels_2d = np.zeros((n_params, n_params, panel_res, panel_res))\n",
    "\n",
    "for c_idx, comb in enumerate(param_combs):\n",
    "    p0, p1 = comb\n",
    "    p0_idx, p1_idx = list(ld_ranges.keys()).index(p0), list(ld_ranges.keys()).index(p1)\n",
    "    density_panels_2d[p0_idx, p1_idx] = density_panels[c_idx].T\n",
    "    density_panels_2d[p1_idx, p0_idx] = density_panels[c_idx]\n",
    "    mean_panels_2d[p0_idx, p1_idx] = mean_panels[c_idx].T\n",
    "    mean_panels_2d[p1_idx, p0_idx] = mean_panels[c_idx]\n",
    "    min_panels_2d[p0_idx, p1_idx] = min_panels[c_idx].T\n",
    "    min_panels_2d[p1_idx, p0_idx] = min_panels[c_idx]\n",
    "    max_panels_2d[p0_idx, p1_idx] = max_panels[c_idx].T\n",
    "    max_panels_2d[p1_idx, p0_idx] = max_panels[c_idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6a687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_ds = xr.Dataset(\n",
    "    data_vars = {\n",
    "        \"density\": ([\"param_y\", \"param_x\", \"y\", \"x\"], density_panels_2d),\n",
    "        \"mean\": ([\"param_y\", \"param_x\", \"y\", \"x\"], mean_panels_2d),\n",
    "        \"min\": ([\"param_y\", \"param_x\", \"y\", \"x\"], min_panels_2d),\n",
    "        \"max\": ([\"param_y\", \"param_x\", \"y\", \"x\"], max_panels_2d)\n",
    "    },\n",
    "    coords = {\n",
    "        \"param_y\": list(ld_ranges.keys()),\n",
    "        \"param_x\": list(ld_ranges.keys()),\n",
    "    }\n",
    ")\n",
    "panel_ds.to_netcdf(\"ld_optical_panels1.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb27dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
